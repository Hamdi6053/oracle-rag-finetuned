import os
import re
import pickle
import uuid
from typing import List, Dict, Any, Tuple

# LangChain'in en gÃ¼ncel ve doÄŸru import yollarÄ±
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import (
    MarkdownHeaderTextSplitter,
    RecursiveCharacterTextSplitter,
)

# =================================================================
# BÃ–LÃœM 1: TEMÄ°ZLEME FONKSÄ°YONLARI
# =================================================================

def clean_markdown_content(text: str) -> str:
    """
    PDF'den dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ Markdown iÃ§eriÄŸini temizler:
    - AÅŸÄ±rÄ± boÅŸluklarÄ± kaldÄ±rÄ±r
    - Bozuk tablo formatlarÄ±nÄ± dÃ¼zeltir  
    - Resim referanslarÄ±nÄ± temizler
    - Gereksiz satÄ±r sonlarÄ±nÄ± dÃ¼zenler
    """
    
    # 1. Resim referanslarÄ±nÄ± kaldÄ±r
    text = re.sub(r'!\[\]\([^)]*\)', '', text)
    text = re.sub(r'!\[.*?\]\([^)]*\)', '', text)
    
    # 2. Span etiketlerini temizle ama iÃ§eriÄŸi koru
    text = re.sub(r'<span[^>]*>(.*?)</span>', r'\1', text)
    
    # 3. AÅŸÄ±rÄ± boÅŸluklarÄ± temizle (5+ boÅŸluk â†’ 1 boÅŸluk)
    text = re.sub(r' {5,}', ' ', text)
    
    # 4. Ã‡oklu newline'larÄ± dÃ¼zenle (3+ â†’ 2)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    # 5. SatÄ±r sonundaki boÅŸluklarÄ± temizle
    text = re.sub(r' +\n', '\n', text)
    
    # 6. Bozuk tablo satÄ±rlarÄ±nÄ± temizle
    text = re.sub(r'\|\s*\n\s*\|', '|', text)
    
    # 7. BoÅŸ tablo hÃ¼crelerini dÃ¼zenle
    text = re.sub(r'\|\s+\|', '| |', text)
    
    # 8. Gereksiz **Contents** tablolarÄ±nÄ± kaldÄ±r (iÃ§erik listesi genelde gereksiz)
    text = re.sub(r'\*\*Contents\*\*.*?\n\|.*?\n(\|.*?\n)*', '', text, flags=re.DOTALL)
    
    return text.strip()

def is_content_meaningful(content: str) -> bool:
    """
    GeliÅŸmiÅŸ iÃ§erik anlamlÄ±lÄ±k kontrolÃ¼ - Web araÅŸtÄ±rmasÄ± sonucu iyileÅŸtirildi.
    
    Kontrol edilenler:
    - Minimum uzunluk (100 karakter)
    - CÃ¼mle sayÄ±sÄ± (en az 1 tam cÃ¼mle)
    - AnlamlÄ± kelime oranÄ±
    - Header-only detection (geliÅŸmiÅŸ)
    - Ä°Ã§erik Ã§eÅŸitliliÄŸi
    - Tablo separator kontrolÃ¼
    """
    content = content.strip()
    
    # 1. âš¡ Minimum uzunluk kontrolÃ¼ (50 â†’ 100 karakter)
    if len(content) < 100:
        return False
    
    # 2. ğŸš« Sadece tablo separator'larÄ±
    if re.match(r'^[\|\-\s\+\=]+$', content):
        return False
    
    # 3. ğŸš« GeliÅŸmiÅŸ header-only detection
    # Sadece markdown header'larÄ± (#, ##, ###) ve minimal iÃ§erik
    header_pattern = r'^[#\s\*\-\_\[\]]+[\w\s\*\-\_\[\]]*[#\s\*\-\_\[\]]*$'
    if re.match(header_pattern, content):
        return False
    
    # 4. ğŸ“ CÃ¼mle sayÄ±sÄ± kontrolÃ¼
    # En az 1 tam cÃ¼mle olmalÄ± (nokta, Ã¼nlem, soru iÅŸareti ile biten)
    sentences = re.findall(r'[.!?]+', content)
    if len(sentences) == 0:
        return False
    
    # 5. ğŸ”¤ AnlamlÄ± kelime kontrolÃ¼
    # Sadece harflerden oluÅŸan kelimeleri say
    words = re.findall(r'\b[a-zA-ZÃ§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄIÄ°Ã–ÅÃœ]+\b', content)
    if len(words) < 8:  # En az 8 anlamlÄ± kelime
        return False
    
    # 6. ğŸ“Š Ä°Ã§erik Ã§eÅŸitliliÄŸi kontrolÃ¼
    # Sadece tekrarlanan kelimelerden oluÅŸuyorsa red
    unique_words = set(word.lower() for word in words)
    if len(unique_words) < 5:  # En az 5 farklÄ± kelime
        return False
    
    # 7. ğŸ¯ Alfanumerik karakter oranÄ± kontrolÃ¼
    # Ä°Ã§eriÄŸin en az %40'Ä± alfanumerik olmalÄ±
    alphanum_chars = len(re.findall(r'[a-zA-Z0-9Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄIÄ°Ã–ÅÃœ]', content))
    total_chars = len(content)
    if alphanum_chars / total_chars < 0.4:
        return False
    
    # 8. ğŸš« Sadece liste Ã¶ÄŸeleri kontrolÃ¼
    # Sadece "- item", "* item", "1. item" formatÄ±nda olan iÃ§erikleri red
    list_only_pattern = r'^[\s\-\*\d\.\)]+[\w\s\-\*\d\.\)]*$'
    if re.match(list_only_pattern, content) and len(sentences) == 0:
        return False
    
    # 9. âœ… Paragraph varlÄ±ÄŸÄ± kontrolÃ¼
    # En az bir paragraph (Ã§oklu cÃ¼mle) olmalÄ±
    paragraphs = content.split('\n\n')
    meaningful_paragraphs = [p for p in paragraphs if len(p.strip()) > 30 and '.' in p]
    if len(meaningful_paragraphs) == 0:
        return False
    
    return True

# =================================================================
# BÃ–LÃœM 2: PICKLE STORE SINIFI (deÄŸiÅŸiklik yok)
# =================================================================
class SimplePickleStore:
    """Basit pickle tabanlÄ± kalÄ±cÄ± depolama"""
    
    def __init__(self, storage_path: str):
        self.storage_path = storage_path
        os.makedirs(storage_path, exist_ok=True)
        self.store_file = os.path.join(storage_path, "documents.pkl")
        self._store = self._load_store()
    
    def _load_store(self) -> Dict:
        if os.path.exists(self.store_file):
            try:
                with open(self.store_file, 'rb') as f:
                    return pickle.load(f)
            except:
                print("Store dosyasÄ± okunamadÄ±, yeni store oluÅŸturuluyor.")
        return {}
    
    def _save_store(self):
        with open(self.store_file, 'wb') as f:
            pickle.dump(self._store, f)
    
    def mget(self, keys: List[str]) -> List[Document]:
        return [self._store.get(key) for key in keys]
    
    def mset(self, key_value_pairs: List[tuple]):
        for key, value in key_value_pairs:
            self._store[key] = value
        self._save_store()
    
    def mdelete(self, keys: List[str]):
        for key in keys:
            self._store.pop(key, None)
        self._save_store()

# =================================================================
# BÃ–LÃœM 3: TEMÄ°ZLENMÄ°Å CHUNKING STRATEJÄ°SÄ°
# =================================================================

def clean_text(text: str) -> str:
    """BaÅŸlÄ±k metinlerini temizler."""
    text = re.sub(r'<.*?>', '', text)  # HTML taglarÄ±
    text = re.sub(r'(\*\*|__|\*|_)', '', text)  # Markdown formatlarÄ±
    text = re.sub(r'\s+', ' ', text)  # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
    return text.strip()

def create_cleaned_parent_child_sections(
    md_text: str,
    source_file: str,
    max_child_length: int = 1200,  # Biraz daha kÃ¼Ã§Ã¼k chunk'lar
) -> Tuple[List[Tuple[str, Document]], List[Document]]:
    """
    TemizlenmiÅŸ ve optimize edilmiÅŸ parent-child chunking.
    """
    
    print("ğŸ§¹ Markdown iÃ§eriÄŸi temizleniyor...")
    cleaned_text = clean_markdown_content(md_text)
    print(f"âœ… Ä°Ã§erik temizlendi: {len(md_text)} â†’ {len(cleaned_text)} karakter")
    
    # MarkdownHeaderTextSplitter konfigÃ¼rasyonu
    headers_to_split_on = [
        ("#", "h1"),           
        ("##", "h2"),          
        ("###", "h3"),         
        ("####", "h4"),        
    ]
    
    print("ğŸ”„ MarkdownHeaderTextSplitter ile bÃ¶lme...")
    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on,
        strip_headers=False,  
        return_each_line=False
    )
    
    md_header_splits = markdown_splitter.split_text(cleaned_text)
    print(f"âœ… BaÅŸlÄ±k bazlÄ± {len(md_header_splits)} bÃ¶lÃ¼m oluÅŸturuldu")
    
    # AnlamlÄ± olmayan bÃ¶lÃ¼mleri filtrele
    meaningful_splits = []
    filtered_count = 0
    
    for doc in md_header_splits:
        if is_content_meaningful(doc.page_content):
            meaningful_splits.append(doc)
        else:
            filtered_count += 1
    
    print(f"ğŸš® {filtered_count} anlamsÄ±z bÃ¶lÃ¼m filtrelendi")
    print(f"âœ… {len(meaningful_splits)} anlamlÄ± bÃ¶lÃ¼m kaldÄ±")
    
    if not meaningful_splits:
        print("âš ï¸ HiÃ§ anlamlÄ± bÃ¶lÃ¼m bulunamadÄ±, fallback stratejisi...")
        return _fallback_single_parent(cleaned_text, source_file, max_child_length)
    
    parents_for_store: List[Tuple[str, Document]] = []
    all_child_docs: List[Document] = []
    
    # H1 ve H2 bazlÄ± parent grouping (iyileÅŸtirilmiÅŸ)
    current_parent_content = ""
    current_parent_title = "BaÅŸlangÄ±Ã§ BÃ¶lÃ¼mÃ¼"
    current_parent_level = "h1"
    parent_docs_buffer = []
    
    for doc in meaningful_splits:
        doc_metadata = doc.metadata
        doc_content = doc.page_content.strip()
        
        # En yÃ¼ksek seviye baÅŸlÄ±ÄŸÄ± bul
        header_level = _get_highest_header_level(doc_metadata)
        header_title = _get_primary_header_title(doc_metadata)
        
        # H1 veya H2 seviyesinde yeni parent baÅŸlatÄ±lÄ±yor mu?
        if header_level in ['h1', 'h2']:
            # Ã–nceki parent'Ä± kaydet (eÄŸer anlamlÄ±ysa)
            if current_parent_content.strip() and is_content_meaningful(current_parent_content):
                parent_id = str(uuid.uuid4())
                parent_doc = Document(
                    page_content=current_parent_content.strip(),
                    metadata={
                        "source": source_file,
                        "header_hierarchy": current_parent_title,
                        "section_level": current_parent_level,
                        "content_length": len(current_parent_content.strip()),
                        "is_cleaned": True,  # TemizlenmiÅŸ olduÄŸunu iÅŸaretle
                    }
                )
                parents_for_store.append((parent_id, parent_doc))
                
                # Bu parent'Ä±n child'larÄ±nÄ± oluÅŸtur
                child_docs = _create_cleaned_child_chunks(
                    parent_docs_buffer, current_parent_title, parent_id, source_file, max_child_length
                )
                all_child_docs.extend(child_docs)
            
            # Yeni parent baÅŸlat
            current_parent_content = doc_content
            current_parent_title = header_title or f"BÃ¶lÃ¼m {len(parents_for_store) + 1}"
            current_parent_level = header_level
            parent_docs_buffer = [doc]
            
        else:
            # Mevcut parent'a ekle (H3+ seviyesi)
            current_parent_content += "\n\n" + doc_content
            parent_docs_buffer.append(doc)
    
    # Son parent'Ä± kaydet
    if current_parent_content.strip() and is_content_meaningful(current_parent_content):
        parent_id = str(uuid.uuid4())
        parent_doc = Document(
            page_content=current_parent_content.strip(),
            metadata={
                "source": source_file,
                "header_hierarchy": current_parent_title,
                "section_level": current_parent_level,
                "content_length": len(current_parent_content.strip()),
                "is_cleaned": True,
            }
        )
        parents_for_store.append((parent_id, parent_doc))
        
        child_docs = _create_cleaned_child_chunks(
            parent_docs_buffer, current_parent_title, parent_id, source_file, max_child_length
        )
        all_child_docs.extend(child_docs)
    
    print(f"âœ… {len(parents_for_store)} parent ve {len(all_child_docs)} child dokÃ¼man oluÅŸturuldu")
    return parents_for_store, all_child_docs

def _get_highest_header_level(metadata: Dict) -> str:
    """Metadata'dan en yÃ¼ksek seviye baÅŸlÄ±ÄŸÄ± bul"""
    for level in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
        if level in metadata:
            return level
    return 'h0'

def _get_primary_header_title(metadata: Dict) -> str:
    """Metadata'dan birincil baÅŸlÄ±k metnini al"""
    for level in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
        if level in metadata and metadata[level]:
            return clean_text(metadata[level])
    return "BaÅŸlÄ±ksÄ±z BÃ¶lÃ¼m"

def _create_cleaned_child_chunks(
    parent_sections: List[Document],
    parent_title: str,
    parent_id: str, 
    source_file: str,
    max_length: int
) -> List[Document]:
    """
    TemizlenmiÅŸ child chunk'lar oluÅŸturur.
    """
    child_docs = []
    
    for section_idx, section in enumerate(parent_sections):
        section_content = section.page_content.strip()
        section_metadata = section.metadata
        
        # AnlamlÄ± iÃ§erik kontrolÃ¼
        if not is_content_meaningful(section_content):
            continue
        
        section_title = _get_primary_header_title(section_metadata)
        section_level = _get_highest_header_level(section_metadata)
        
        # Ä°Ã§erik bÃ¶lme (temizlenmiÅŸ versiyon)
        if len(section_content) <= max_length:
            chunks = [section_content]
        else:
            chunks = _split_content_cleaned(section_content, max_length)
        
        # Her chunk iÃ§in child dokÃ¼man oluÅŸtur
        for chunk_idx, chunk in enumerate(chunks):
            # Son bir kez anlamlÄ±lÄ±k kontrolÃ¼
            if not is_content_meaningful(chunk):
                continue
                
            hierarchy = f"{parent_title} > {section_title}" if section_title != parent_title else parent_title
            
            child_meta = {
                "source": source_file,
                "header_hierarchy": hierarchy,
                "section_level": section_level,
                "parent_id": parent_id,
                "chunk_index": chunk_idx,
                "subsection_title": section_title,
                "content_length": len(chunk),
                "is_cleaned": True,
                **{k: v for k, v in section_metadata.items() if k not in ['source', 'header_hierarchy']}
            }
            
            child_docs.append(Document(page_content=chunk, metadata=child_meta))
    
    print(f"   ğŸ“ {parent_title}: {len(child_docs)} anlamlÄ± child chunk oluÅŸturuldu")
    return child_docs

def _split_content_cleaned(content: str, max_length: int) -> List[str]:
    """
    TemizlenmiÅŸ iÃ§erik bÃ¶lme stratejisi.
    """
    if len(content) <= max_length:
        return [content]
    
    # RecursiveCharacterTextSplitter ile daha dikkatli bÃ¶lme
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=max_length,
        chunk_overlap=150,  # BaÄŸlam korunmasÄ± iÃ§in overlap
        length_function=len,
        separators=[
            "\n\n",      # Paragraf arasÄ±
            "\n",        # SatÄ±r sonu
            ". ",        # CÃ¼mle sonu
            "! ",        # Ãœnlem
            "? ",        # Soru
            "; ",        # NoktalÄ± virgÃ¼l
            " ",         # BoÅŸluk
            ""           # Karakter bazlÄ±
        ]
    )
    
    chunks = text_splitter.split_text(content)
    
    # AnlamlÄ± olmayan chunk'larÄ± filtrele
    meaningful_chunks = []
    for chunk in chunks:
        chunk = chunk.strip()
        if is_content_meaningful(chunk):
            meaningful_chunks.append(chunk)
    
    return meaningful_chunks

def _fallback_single_parent(md_text: str, source_file: str, max_child_length: int) -> Tuple[List[Tuple[str, Document]], List[Document]]:
    """BaÅŸlÄ±k bulunamadÄ±ÄŸÄ±nda fallback stratejisi (temizlenmiÅŸ)"""
    cleaned_text = clean_markdown_content(md_text)
    
    parent_id = str(uuid.uuid4())
    parent_doc = Document(
        page_content=cleaned_text.strip(),
        metadata={
            "source": source_file,
            "header_hierarchy": "Tam DokÃ¼man",
            "section_level": "h0",
            "content_length": len(cleaned_text.strip()),
            "is_cleaned": True,
        }
    )
    
    # TÃ¼m dokÃ¼manÄ± anlamlÄ± child chunk'lara bÃ¶l
    chunks = _split_content_cleaned(cleaned_text, max_child_length)
    child_docs = []
    
    for idx, chunk in enumerate(chunks):
        child_meta = {
            "source": source_file,
            "header_hierarchy": "Tam DokÃ¼man > BÃ¶lÃ¼m",
            "section_level": "chunk",
            "parent_id": parent_id,
            "chunk_index": idx,
            "content_length": len(chunk),
            "is_cleaned": True,
        }
        child_docs.append(Document(page_content=chunk, metadata=child_meta))
    
    return [(parent_id, parent_doc)], child_docs

# =================================================================
# ANA PROGRAM AKIÅI
# =================================================================

# 1. VERÄ°YÄ° YÃœKLEME
markdown_file_path = "./using-manufacturing.md"
markdown_content = ""
source_filename = ""

try:
    with open(markdown_file_path, "r", encoding='utf-8') as f:
        markdown_content = f.read()
    source_filename = os.path.basename(markdown_file_path)
    print(f"âœ… '{source_filename}' dosyasÄ± baÅŸarÄ±yla okundu ({len(markdown_content)} karakter)")
except FileNotFoundError:
    print(f"âŒ HATA: Dosya bulunamadÄ±! LÃ¼tfen yolu kontrol et: {markdown_file_path}")
    exit()
except Exception as e:
    print(f"âŒ Dosya okuma hatasÄ±: {e}")
    exit()

# 2. TEMÄ°ZLENMÄ°Å Ä°NDEKSLEME
if markdown_content:
    print("\nğŸš€ TemizlenmiÅŸ Markdown Chunking BaÅŸlÄ±yor")
    print("=" * 60)

    # Embedding Modelini YÃ¼kle
    print("ğŸ“¥ Embedding modeli (BAAI/bge-base-en) yÃ¼kleniyor...")
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-base-en",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    print("âœ… Embedding modeli yÃ¼klendi")

    # KalÄ±cÄ± Depolama AlanlarÄ±nÄ± Ayarla
    print("ğŸ’¾ Depolama alanlarÄ± yapÄ±landÄ±rÄ±lÄ±yor...")
    vectorstore = Chroma(
        collection_name="oracle_md_enhanced_v2",  # GeliÅŸmiÅŸ filtreleme ile temizlenmiÅŸ koleksiyon
        embedding_function=embeddings,
        persist_directory="./oracle_vector_db_md_enhanced"
    )
    pickle_store = SimplePickleStore("./parent_document_store_md_enhanced")
    print("âœ… Depolama alanlarÄ± hazÄ±r")

    # TEMÄ°ZLENMÄ°Å CHUNKING
    print("\nğŸ§¹ TemizlenmiÅŸ MarkdownHeaderTextSplitter ile bÃ¶lme baÅŸlÄ±yor...")
    
    parent_docs_for_store, child_docs_for_embedding = create_cleaned_parent_child_sections(
        markdown_content, 
        source_filename,
        max_child_length=1200  # Biraz daha kÃ¼Ã§Ã¼k chunk'lar
    )
    
    print(f"\nğŸ“Š TEMÄ°ZLENMÄ°Å SONUÃ‡LAR:")
    print(f"   ğŸ“š Parent dokÃ¼manlar: {len(parent_docs_for_store)} adet")
    print(f"   ğŸ“„ Child dokÃ¼manlar: {len(child_docs_for_embedding)} adet")
    
    # Kalite kontrol
    if parent_docs_for_store:
        total_parent_length = sum(len(doc.page_content) for _, doc in parent_docs_for_store)
        avg_parent_length = total_parent_length / len(parent_docs_for_store)
        print(f"   ğŸ“ Ortalama parent uzunluÄŸu: {avg_parent_length:.0f} karakter")
        
    if child_docs_for_embedding:
        total_child_length = sum(len(doc.page_content) for doc in child_docs_for_embedding)
        avg_child_length = total_child_length / len(child_docs_for_embedding)
        print(f"   ğŸ“ Ortalama child uzunluÄŸu: {avg_child_length:.0f} karakter")

    # Kalite kontrol: En kÄ±sa ve en uzun chunk'larÄ± gÃ¶ster
    if child_docs_for_embedding:
        lengths = [len(doc.page_content) for doc in child_docs_for_embedding]
        print(f"   ğŸ“ En kÄ±sa chunk: {min(lengths)} karakter")
        print(f"   ğŸ“ En uzun chunk: {max(lengths)} karakter")

    # DokÃ¼manlarÄ± KalÄ±cÄ± Olarak Kaydet
    if child_docs_for_embedding and parent_docs_for_store:
        print("\nğŸ’¾ TemizlenmiÅŸ dokÃ¼mantlar kaydediliyor...")
        
        pickle_store.mset(parent_docs_for_store)
        print(f"âœ… {len(parent_docs_for_store)} parent dokÃ¼man kaydedildi")
        
        vectorstore.add_documents(child_docs_for_embedding)
        print(f"âœ… {len(child_docs_for_embedding)} child dokÃ¼man vektÃ¶rleÅŸtirildi")
        
        print("\nğŸ‰ TemizlenmiÅŸ indeksleme tamamlandÄ±!")
        print("ğŸ’¡ Yeni koleksiyon: oracle_md_cleaned_v1")
        print("ğŸ’¡ 2_query.py'de COLLECTION_NAME'i gÃ¼ncelleyebilirsiniz")
        
    else:
        print("âŒ Ä°ÅŸlenecek dokÃ¼man bulunamadÄ±!")
else:
    print("âŒ Markdown iÃ§erik boÅŸ, indeksleme atlandÄ±")
