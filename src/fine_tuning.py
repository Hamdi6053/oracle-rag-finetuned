!pip uninstall -y bitsandbytes
!pip install -U bitsandbytes
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps "xformers<0.0.26" trl peft accelerate

print("KÃ¼tÃ¼phaneler yÃ¼klendi. Åimdi 'Runtime' -> 'Restart runtime' yapÄ±n ve sonraki hÃ¼creyi Ã§alÄ±ÅŸtÄ±rÄ±n.")
import os
import torch
from datasets import Dataset
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import FastLanguageModel
import json
import re
try:
    import bitsandbytes as bnb
    print(f"âœ“ Bitsandbytes sÃ¼rÃ¼mÃ¼: {bnb.__version__}")
except ImportError:
    print("âŒ Bitsandbytes kurulu deÄŸil! LÃ¼tfen oturumu yeniden baÅŸlatÄ±n.")
MODEL_NAME = "Qwen/Qwen2-1.5B-Instruct"
DATASET_FILE_PATH = "/kaggle/input/questionanswer/test_qa.json"
OUTPUT_ADAPTER_DIR = "dora-finetuned-qwen2-1.5b-oracle"
MAX_SEQ_LENGTH = 2048
DORA_RANK = 16
DORA_ALPHA = 32
def load_data_from_custom_format(filepath):
    """
    JSON dosyasÄ±nÄ± okur ve question/answer formatÄ±nÄ± input/output formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
    except FileNotFoundError:
        print(f"HATA: Veri seti dosyasÄ± bulunamadÄ±: {filepath}")
        return []

    data = []
    success_count = 0
    error_count = 0
    
    try:
        parsed_data = json.loads(content)
        if isinstance(parsed_data, list):
            for item in parsed_data:
                if 'question' in item and 'answer' in item:
                    data.append({
                        'input': item['question'],
                        'output': item['answer']
                    })
                    success_count += 1
                else:
                    error_count += 1
    except json.JSONDecodeError:
        print("JSON array formatÄ± deÄŸil, JSONL formatÄ± deneniyor...")
        for line_num, line in enumerate(content.splitlines(), 1):
            line = line.strip()
            if not line:
                continue
            try:
                parsed = json.loads(line)
                if 'question' in parsed and 'answer' in parsed:
                    data.append({
                        'input': parsed['question'],
                        'output': parsed['answer']
                    })
                    success_count += 1
                else:
                    error_count += 1
                    if error_count <= 5:
                        print(f"SatÄ±r {line_num}: 'question' ve 'answer' alanlarÄ± bulunamadÄ±")
            except json.JSONDecodeError as e:
                error_count += 1
                if error_count <= 5:
                    print(f"SatÄ±r {line_num} ayrÄ±ÅŸtÄ±rÄ±lamadÄ±: {str(e)[:100]}...")
        
        if not data:
            print("JSONL formatÄ± da Ã§alÄ±ÅŸmadÄ±, ```json bloklarÄ± deneniyor...")
            json_blocks = re.findall(r'```json\s*\n(.*?)\n```', content, re.DOTALL)
            
            for i, block in enumerate(json_blocks, 1):
                try:
                    cleaned_block = block.strip()
                    parsed = json.loads(cleaned_block)
                    if 'question' in parsed and 'answer' in parsed:
                        data.append({
                            'input': parsed['question'],
                            'output': parsed['answer']
                        })
                        success_count += 1
                    else:
                        error_count += 1
                        if error_count <= 5:
                            print(f"Blok {i}: 'question' ve 'answer' alanlarÄ± bulunamadÄ±")
                except json.JSONDecodeError as e:
                    error_count += 1
                    if error_count <= 5:
                        print(f"Blok {i} ayrÄ±ÅŸtÄ±rÄ±lamadÄ±: {str(e)[:100]}...")
                
    print(f"\nâœ“ BaÅŸarÄ±lÄ±: {success_count} Ã¶rnek")
    print(f"âŒ HatalÄ±: {error_count} Ã¶rnek")
    
    if not data:
        raise ValueError("HATA: HiÃ§bir geÃ§erli Ã¶rnek yÃ¼klenemedi!")
        
    return data
print("Veri seti yÃ¼kleniyor...")
json_data = load_data_from_custom_format(DATASET_FILE_PATH)
dataset = Dataset.from_list(json_data)

print(f"\nğŸ“Š Veri seti Ã¶zeti:")
print(f"Toplam Ã¶rnek sayÄ±sÄ±: {len(dataset)}")
print(f"Ä°lk Ã¶rnek: {dataset[0]}")
print(f"\nğŸ¤– Model yÃ¼kleniyor: {MODEL_NAME}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name()}")
    print(f"Mevcut VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

try:
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=MODEL_NAME,
        max_seq_length=MAX_SEQ_LENGTH,
        dtype=None,
        load_in_4bit=True,
    )
    print("âœ“ Model baÅŸarÄ±yla yÃ¼klendi!")
except Exception as e:
    print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
    print("Alternatif model denenecek...")
    
    try:
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name="unsloth/llama-3.2-3b-instruct-bnb-4bit",  # Alternatif model
            max_seq_length=MAX_SEQ_LENGTH,
            dtype=None,
            load_in_4bit=True,
        )
        print("âœ“ Alternatif model baÅŸarÄ±yla yÃ¼klendi!")
    except Exception as e2:
        print(f"âŒ Alternatif model de yÃ¼klenemedi: {e2}")
        raise
print("\nğŸ”§ DoRA adaptÃ¶rleri ekleniyor...")

model = FastLanguageModel.get_peft_model(
    model,
    r=DORA_RANK,
    lora_alpha=DORA_ALPHA,
    lora_dropout=0.1,
    bias="none",
    use_gradient_checkpointing="unsloth",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                   "gate_proj", "up_proj", "down_proj"],
    use_dora=True,
)

print("âœ“ DoRA adaptÃ¶rleri eklendi!")
model.print_trainable_parameters()
def formatting_prompts_func(examples):
    """
    Oracle-specific formatting function.
    input/output alanlarÄ±nÄ± kullanarak chat formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    """
    inputs = examples["input"]
    outputs = examples["output"]
    texts = []
    
    for input_text, output_text in zip(inputs, outputs):
        messages = [
            {"role": "system", "content": "You are an Oracle Fusion Cloud SCM expert assistant. Provide accurate and helpful information about Oracle manufacturing, supply chain management, and related processes."},
            {"role": "user", "content": input_text},
            {"role": "assistant", "content": output_text},
        ]
        
        try:
            formatted = tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=False
            )
            texts.append(formatted)
        except Exception as e:
            print(f"Chat template hatasÄ±, basit format kullanÄ±lÄ±yor: {e}")
            simple_format = f"<|system|>You are an Oracle Fusion Cloud SCM expert assistant. Provide accurate and helpful information about Oracle manufacturing, supply chain management, and related processes.<|end|>\n<|user|>{input_text}<|end|>\n<|assistant|>{output_text}<|end|>"
            texts.append(simple_format)
    
    return {"text": texts}
print("\nğŸ“ Veri seti formatlanÄ±yor...")
dataset = dataset.map(formatting_prompts_func, batched=True)
print("âœ“ Veri seti formatlandÄ±!")
print(f"\nFormatlanmÄ±ÅŸ Ã¶rnek:\n{dataset[0]['text'][:500]}...")
print("\nğŸš€ EÄŸitim baÅŸlatÄ±lÄ±yor...")
training_args = TrainingArguments(
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    warmup_steps=10,
    num_train_epochs=2,
    learning_rate=1e-4,
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),
    logging_steps=1,
    optim="adamw_8bit",
    weight_decay=0.01,
    lr_scheduler_type="cosine",
    seed=42,
    output_dir="outputs",
    report_to="none",
    save_steps=100,
    save_total_limit=3,
    dataloader_pin_memory=False,
    group_by_length=True,
)
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=MAX_SEQ_LENGTH,
    args=training_args,
    packing=False,
    dataset_num_proc=2,
)
print("=" * 50)
print("EÄÄ°TÄ°M BAÅLIYOR...")
print("=" * 50)

trainer_stats = trainer.train()

print("=" * 50)
print("EÄÄ°TÄ°M TAMAMLANDI!")
print("=" * 50)
print(f"EÄŸitim istatistikleri: {trainer_stats}")
print(f"\nğŸ’¾ Model kaydediliyor: {OUTPUT_ADAPTER_DIR}")

model.save_pretrained(OUTPUT_ADAPTER_DIR)
tokenizer.save_pretrained(OUTPUT_ADAPTER_DIR)

print(f"âœ… Ä°ÅŸlem tamamlandÄ±!")
print(f"ğŸ“ AdaptÃ¶rler '/kaggle/working/{OUTPUT_ADAPTER_DIR}' klasÃ¶rÃ¼ne kaydedildi.")
print("\nğŸ§ª HÄ±zlÄ± test:")
FastLanguageModel.for_inference(model)
test_questions = [
    "What is the purpose of Oracle Fusion Cloud SCM Using Manufacturing?",
    "How do you implement resource allocation in Oracle Fusion Cloud SCM?",
    "What are the key features of Oracle Fusion Cloud SCM in relation to supply chain optimization?"
]

for test_question in test_questions[:2]:  # Ä°lk 2 soruyu test et
    test_messages = [
        {"role": "system", "content": "You are an Oracle Fusion Cloud SCM expert assistant."},
        {"role": "user", "content": test_question},
    ]

    try:
        test_prompt = tokenizer.apply_chat_template(
            test_messages,
            tokenize=False,
            add_generation_prompt=True 
        )
    except:
        test_prompt = f"<|system|>You are an Oracle Fusion Cloud SCM expert assistant.<|end|>\n<|user|>{test_question}<|end|>\n<|assistant|>"

    inputs = tokenizer([test_prompt], return_tensors="pt").to("cuda")

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            use_cache=True,
            temperature=0.3,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"\nğŸ” Soru: {test_question}")

    # CevabÄ± temizle
    assistant_marker = 'assistant'
    if assistant_marker in response:
        clean_response = response.split(assistant_marker)[-1].strip()
        if clean_response.startswith('>'):
            clean_response = clean_response[1:].strip()
    else:
        clean_response = response.split(test_question)[-1].strip()

    print(f"ğŸ¤– Cevap: {clean_response}")
    print("-" * 80)

print("\nğŸ‰ TÃ¼m iÅŸlemler baÅŸarÄ±yla tamamlandÄ±!")
print(f"ğŸ“ˆ Toplam {len(dataset)} Ã¶rnek ile {training_args.num_train_epochs} epoch eÄŸitim tamamlandÄ±.")
print(f"ğŸ’¾ Model '/kaggle/working/{OUTPUT_ADAPTER_DIR}' klasÃ¶rÃ¼ne kaydedildi.")

import torch
from transformers import AutoTokenizer
from peft import PeftModel
from unsloth import FastLanguageModel
import os
import gc
base_model_name = "Qwen/Qwen2-1.5B-Instruct"
adapter_path = "/kaggle/working/dora-finetuned-qwen2-1.5b-oracle"
merged_model_path = "/kaggle/working/oracle-qwen2-1.5b-merged-final"
hub_repo_name = "ozkurt7/oracle-qwen2-1.5b-merged-final"
from dotenv import load_dotenv
import os
load_dotenv()
hf_token = os.getenv("HF_TOKEN")
print("="*60 + "\nMODEL BÄ°RLEÅTÄ°RME (CPU)\n" + "="*60)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=base_model_name,
    max_seq_length=2048,
    dtype=torch.float16,
    load_in_4bit=False,
    device_map="cpu",
)
model = PeftModel.from_pretrained(model, adapter_path)
model = model.merge_and_unload()
print("âœ… Model baÅŸarÄ±yla birleÅŸtirildi!")
gc.collect()
print("\n" + "="*60 + "\nBÄ°RLEÅTÄ°RÄ°LMÄ°Å MODELÄ° HUB'A YÃœKLEME\n" + "="*60)
try:
    print(f"ğŸ’¾ Model '{merged_model_path}' yoluna kaydediliyor...")
    model.save_pretrained(merged_model_path)
    tokenizer.save_pretrained(merged_model_path)
    print(f"âœ… Model yerel olarak kaydedildi.")
    
    print(f"ğŸŒ Model Hub'a yÃ¼kleniyor: {hub_repo_name}")
    tokenizer.push_to_hub(hub_repo_name, token=hf_token)
    model.push_to_hub(hub_repo_name, token=hf_token)
    print(f"âœ… Model baÅŸarÄ±yla Hub'a yÃ¼klendi: https://huggingface.co/{hub_repo_name}")

except Exception as e:
    print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")

print("\n" + "="*60 + "\nğŸ‰ Ä°ÅLEM TAMAMLANDI!\n" + "="*60)